---
title: "COVID-19 deaths"
output:
  html_document:
    df_print: paged
---

FORECASTING AND TIME SERIES ANALYSIS 

Final Project: Predicting COVID 19 Deaths 

Sophie, Pablo, Gonzalo & Marta 


1.LOAD LIBRARIES
```{r}
library(fpp3)
library(tidyverse)
```

2. DATA LOADING AND PREPARATION 
```{r}
#We read the data
data <- readr::read_csv("COVID-19 Time Series Data.csv")
head(data)

#we select the data that we are interested in
COVID <- data %>% select(Date, Deaths, `Country/Region`) %>% filter(`Country/Region` == "US") 

#as the data is cumulative we make an adjustment to have the deaths per day
for(i in seq(2, nrow(COVID))){
  COVID[i, "Daily_Deaths"] <- COVID[i,"Deaths"] - COVID[(i-1), "Deaths"]
  }
COVID[1, "Daily_Deaths"] <- 0
COVID %>% View()

#we look if we have any missing values before converting the data into a time series
sum(is.na(COVID))

#now we convert the table into a time series
FinalCovid <- COVID %>%
  as_tsibble(index = Date, key = NULL)
```


3. TIME SERIES EXPLORATION

  3.1. Trend
```{r}
FinalCovid %>% autoplot(Daily_Deaths)
```

  3.2. Seasonality
```{r}
dcmp <- FinalCovid %>% model(stl = STL(Daily_Deaths)) 
components(dcmp) %>% autoplot()
```

```{r}
FinalCovid %>% gg_season(Daily_Deaths)
```

4. TRANSFORMATIONS
```{r}
# Is the variance proportional to the level of the series, such that a mathematical transformation is appropriate?
FinalCovid %>% autoplot(Daily_Deaths)
```

We can see in the graph above that the variance is higher as the value increase: the higher is the trend, the higher is the variation in number of death. 
Therefore, the variance is proportional to the level of the series, the relation is multiplicative, so we need to do a transformation. 

We will realize a box-cox transformation:

```{r}
# Transformation of the data 
# the optimal lambda to stabilize the variance: 
FinalCovid %>% features(Daily_Deaths, features = guerrero)
```

lambda very close from 0 so it does not make sense to transform the data 

The bow-cox transformation does not seem relevant but we can try other tranformation: square root, log and cube:

```{r}
# with square root, log , cube
FinalCovid %>% autoplot(sqrt(Daily_Deaths)) 
FinalCovid %>% autoplot(log(Daily_Deaths))
FinalCovid %>% autoplot(Daily_Deaths^(1/3))
```

We can see above that the best transforation is the cubic transformation. We will keep this transformation for the forecast. 

```{r}
# change the daily death variables and cube it: 
FinalCovid <- FinalCovid %>% mutate(Daily_Deaths = Daily_Deaths^(1/3))
FinalCovid%>%tail()

```


5. FORECASTING 

- SIMPLE FORECASTING
- FORECASTING WITH TRANSFORMATION (no need to do any transformation in the forecast)
- FORECASTING WITH DECOMPOSITION
- FORECASTING WITH EXPONENTIAL SMOOTHING

Once we find the bests models we apply cross validation on those and decide on the best model. 

  5.1. Simple forecasting
```{r}
# Data Preparation: Create the train dataset excluding October, November and December. 
TrainData <-FinalCovid %>%
  filter(Date < '2021-10-1')

TrainData%>%autoplot(Daily_Deaths)

```

```{r}
#Train de models
fit_covid <- TrainData %>%
  model(
    Seasonal_naive = SNAIVE(Daily_Deaths),
    Naive = NAIVE(Daily_Deaths),
    Drift = RW(Daily_Deaths ~ drift()),
    Mean = MEAN(Daily_Deaths)
)
```

```{r}
#forecast
forecast_covid <- fit_covid %>%
  forecast(h = 60)
forecast_covid

#visualize forecast
forecast_covid %>%
  autoplot(FinalCovid, level = NULL) +
  guides(colour = guide_legend(title = "Forecast"))+
  labs(title = "Divorce forecast fro 2013 and 2014 with different methods",
       y = "Number of divorces")


```

These simple methods, do not seem to give us an accurate forecast. The Drift method clearly deviates, showing once again the non-homogenous trend our time series follows. The Seasonal Naive method, does capture some predictions correctly however it still deviates. And the mean and the na誰ve method are quite simple not capturing all the complexity of the data.


  5.2. Forecasting with decomposition

As we have mentioned before there is no clear seasonal pattern observable. However, we still want to try to forecast the Covid test taking into account the seasonal adjustment. We will first do it with the drift method including the trend we observed and then with the naive method, which we believe will result in a better forecast given the observations above.  

```{r}
TrainData %>% model(stlf = decomposition_model( STL(Daily_Deaths ~ trend(window = 7), robust = TRUE), RW(season_adjust ~ drift()) )) %>% forecast(h=60) %>% autoplot(FinalCovid)
```
As we observe from the graph the forecast does not correctly manage to predict the future deaths. It includes the trend component which in this case is not necessary. Therefore we will try this same method but combinig the seasonal adjust with the naive method

```{r}
TrainData %>%model(stlf = decomposition_model(STL(Daily_Deaths ~season(window = 7), robust = TRUE),
                                              NAIVE(season_adjust))) %>%
  forecast(h=60) %>%
  autoplot(FinalCovid)
```

As thought at the beginning, the second forecast seems to aproximate more the real data points. However, these forecast do not yet seem very good so lets keep trying other methods learned in class.


  5.3. Forecasting with exponential smoothing
```{r}
fit_expsmo <- TrainData %>%
model(ANN = ETS(Daily_Deaths ~ error("A") + trend("N") + season("N")))
report(fit_expsmo)

components(fit_expsmo) %>% autoplot()

fit_expsmo %>%
forecast(h = 60) %>%
autoplot(FinalCovid)

```
We observe that the alpha used is quite low, indicating that the level is taking into account the mean rather than the independent data points. In fact as we are not introducing any trend or seasonal component the forecast is like the na誰ve method.

As mention before the seasonality and the trend components in this time series are not constant or representative. However, we will try different combination of exponential smoothing to see which one fits best. 

```{r}
fit_expsmo2 <- TrainData %>%
model(ANM = ETS(Daily_Deaths ~ error("A") + trend("N") + season("M")))
report(fit_expsmo2)

components(fit_expsmo2) %>% autoplot()

fit_expsmo2 %>%
forecast(h = 60) %>%
autoplot(FinalCovid)


```

Here we are considering a multiplicative seasonal component with a gamma very close to 0, meaning it stays fixed (mean). We observe that the prediciton is not quite accurate. 

```{r}
fit_expsmo3 <- TrainData %>%
model(MAdA = ETS(Daily_Deaths ~ error("M") + trend("Ad") + season("A")))
report(fit_expsmo3)

components(fit_expsmo3) %>% autoplot()

fit_expsmo3 %>%
forecast(h = 60) %>%
autoplot(FinalCovid)
```

Having a multiplicative error, clearly widens up the forecasting interval. Eventhough it cannot be seen correctly the forecast does not look bad. We will have to further examine this model in the residual analysis.


```{r}
fit_expsmo4 <- TrainData %>%
model(MNA = ETS(Daily_Deaths ~ error("M") + trend("N") + season("A")))
report(fit_expsmo4)

components(fit_expsmo4) %>% autoplot()

fit_expsmo4 %>%
forecast(h = 60) %>%
autoplot(FinalCovid)


```

Despite the forecast not being perfect, it does not look very bad. It could be a good model. 


So now that we have analyzed different method we conclude that the 3 best model that we will test on cross validation are:

* Exponential smoothing model M Ad A
* Exponential smoothing model M N A
* Decomposition model with the seasonal adjusted data combined with na誰ve

Furthermore we will also include the na誰ve method as we believe that the Covid deaths may be quite difficult to predict based on past data and it maybe best to stick with the actual value. 


  5.4. Cross validation

```{r}
#We will create sets, containg first 15 days and then always 5 days more.
COVID_CV <- FinalCovid %>%
stretch_tsibble(.init = 15, .step = 5) %>%
filter(.id != max(.id))

COVID_CV
```

```{r}
#We will know fit our 4 models for the sets
fit_CV <- COVID_CV %>%
  model(
        MAdA = ETS(Daily_Deaths ~ error("M") + trend("Ad") + season("A")),
        MNA = ETS(Daily_Deaths ~ error("M") + trend("N") + season("A")),
        stlf = decomposition_model(STL(Daily_Deaths ~season(window = 7), robust = TRUE),
                                              NAIVE(season_adjust)),
        Naive = NAIVE(Daily_Deaths)
        )
fit_CV

```

```{r}
#forecast
forecast_CV <- fit_CV %>%
  forecast(h=60)
```


6. EVALUATING THE MODELS 

  6.1 Accuracy 
```{r}
forecast_CV %>% accuracy(FinalCovid)
```
MEASURING FORECAST ACCURACY:

Scale Dependent Errors:
MAE is simply, as the name suggests, the mean of the absolute errors. The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average.

MAE tells us how big of an error we have in the forecast: In this case, the stlf model is the best.


As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit.

RMSE then tells us that the best model in terms of accuracy is: STLF

MAPE. The mean absolute percent error (MAPE) expresses accuracy as a percentage of the error. Because the MAPE is a percentage, it can be easier to understand than the other accuracy measure statistics. For example, if the MAPE is 5, on average, the forecast is off by 5%

MAPE tells us that the best model in terms of accuracy is: STLF

An MASE = 0.5, means that our model has doubled the prediction accuracy. The lower, the better. When MASE > 1, that means the model needs a lot of improvement. The Mean Absolute Percentage Error - MAPE, measures the difference of forecast errors and divides it by the actual observation value.

MASE tells us that the best model is: STLF


  6.2 Residual Analysis
  
    6.2.1 First model
```{r}
fitna <- TrainData %>% model(Naive = NAIVE(Daily_Deaths))
fitMada <- TrainData %>% model(ETS(Daily_Deaths ~ error("M") + trend("Ad") + season("A")))
fitmna <- TrainData %>% model(ETS(Daily_Deaths ~ error("M") + trend("N") + season("A")))   
fitstlf <- TrainData %>% model(decomposition_model(STL(Daily_Deaths ~season(window = 7), robust = TRUE),
                                              NAIVE(season_adjust)))
fitna %>% gg_tsresiduals()
fitMada %>% gg_tsresiduals()
fitmna %>% gg_tsresiduals()
fitstlf %>% gg_tsresiduals()
```
Non of the models is actually great. However, as the cross validation method suggested, the best model in terms of accuracy (decomposition model with seasonal adjustment) is also the only model which residuals are not auto correlated and whose distribution aproximates more to normality. Unfortunately, as we have mentioned, the model is still not good since its residuals variance is far from 0.